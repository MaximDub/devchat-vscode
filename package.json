{
	"name": "devchat",
	"displayName": "DevChat",
	"description": "Write prompts, not code",
	"version": "0.1.22",
	"icon": "assets/devchat.png",
	"publisher": "merico",
	"engines": {
		"vscode": "^1.75.0"
	},
	"repository": {
		"type": "git",
		"url": "https://github.com/devchat-ai/devchat-vscode.git"
	},
	"categories": [
		"Programming Languages",
		"Snippets",
		"Machine Learning",
		"Education"
	],
	"keywords": [
		"ai",
		"anthropic",
		"assistant",
		"autocomplete",
		"bot",
		"chat",
		"chatbot",
		"codegen",
		"c#",
		"c++",
		"codex",
		"co-pilot",
		"devchat",
		"documentation",
		"go",
		"golang",
		"intellisense",
		"java",
		"javascript",
		"keybindings",
		"kotlin",
		"llm",
		"model",
		"openai",
		"php",
		"pilot",
		"python",
		"refactor",
		"ruby",
		"sourcegraph",
		"snippets",
		"test",
		"typescript"
	],
	"activationEvents": [
		"*"
	],
	"main": "./dist/extension.js",
	"files": [
		"dist/*",
		"bin/*",
		"assets/*",
		"tools/*",
		"workflows/*",
		"LICENSE",
		"README.md"
	],
	"contributes": {
		"configuration": {
			"title": "DevChat",
			"properties": {
				"devchat.Model.gpt-3-5": {
					"type": "object",
					"properties": {
						"provider": { 
							"type": "string", 
							"default": "openai", 
							"enum": ["openai"],
							"description": "Specify which provider host this llm model"
						},
						"api_key": { 
							"type": "string", 
							"default": "",
							"description": "Specify access key for selected provider." 
						},
						"api_base": { 
							"type": "string", 
							"default": "",
							"description": "Specify the api base for selected provider. Leave it blank if you want to use default api base."
						},
						"temperature": {
							"type": "number",
							"default": 0.3,
							"description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
						},
						"max_tokens": {
							"type": "number",
							"default": 1000,
							"description": "The maximum number of tokens to generate in the chat completion.\nThe total length of input tokens and generated tokens is limited by the model's context length. Example Python code for counting tokens."
						},
						"presence_penalty": {
							"type": "number",
							"default": 0,
							"description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."
						},
						"frequency_penalty": {
							"type": "number",
							"default": 0,
							"description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
						}
					},
					"required": ["provider", "key"],
					"additionalProperties": false,
					"order": 0,
					"markdownDescription": "Specify the properties for gpt-3.5-turbo model. Leave it blank if you won't use this llm model. [how to set?](https://platform.openai.com/docs/api-reference/chat/create#temperature)"
				},
				"devchat.Model.gpt-3-5-16k": {
					"type": "object",
					"properties": {
						"provider": { 
							"type": "string", 
							"default": "openai", 
							"enum": ["openai"],
							"description": "Specify which provider host this llm model"
						},
						"api_key": { 
							"type": "string", 
							"default": "",
							"description": "Specify access key for selected provider." 
						},
						"api_base": { 
							"type": "string", 
							"default": "",
							"description": "Specify the api base for selected provider. Leave it blank if you want to use default api base."
						},
						"temperature": {
							"type": "number",
							"default": 0.3,
							"description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
						},
						"max_tokens": {
							"type": "number",
							"default": 1000,
							"description": "The maximum number of tokens to generate in the chat completion.\nThe total length of input tokens and generated tokens is limited by the model's context length. Example Python code for counting tokens."
						},
						"presence_penalty": {
							"type": "number",
							"default": 0,
							"description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."
						},
						"frequency_penalty": {
							"type": "number",
							"default": 0,
							"description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
						}
					},
					"required": ["provider", "key"],
					"additionalProperties": false,
					"order": 1,
					"markdownDescription": "Specify properties for gpt-3.5-turbo-16k model. Leave it blank if you won't use this llm model. [how to set?](https://platform.openai.com/docs/api-reference/chat/create#temperature) "
				},
				"devchat.Model.gpt-4": {
					"type": "object",
					"properties": {
						"provider": { 
							"type": "string", 
							"default": "openai", 
							"enum": ["openai"],
							"description": "Specify which provider host this llm model"
						},
						"api_key": { 
							"type": "string", 
							"default": "",
							"description": "Specify access key for selected provider." 
						},
						"api_base": { 
							"type": "string", 
							"default": "",
							"description": "Specify the api base for selected provider. Leave it blank if you want to use default api base."
						},
						"temperature": {
							"type": "number",
							"default": 0.3,
							"description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
						},
						"max_tokens": {
							"type": "number",
							"default": 1000,
							"description": "The maximum number of tokens to generate in the chat completion.\nThe total length of input tokens and generated tokens is limited by the model's context length. Example Python code for counting tokens."
						},
						"presence_penalty": {
							"type": "number",
							"default": 0,
							"description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."
						},
						"frequency_penalty": {
							"type": "number",
							"default": 0,
							"description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
						}
					},
					"additionalProperties": false,
					"order": 2,
					"markdownDescription": "properties for gpt-4 model. Leave it blank if you won't use this llm model. [how to set?](https://platform.openai.com/docs/api-reference/chat/create#temperature)"
				},
				"devchat.Model.claude-2": {
					"type": "object",
					"properties": {
						"provider": { 
							"type": "string", 
							"default": "anthropic", 
							"enum": ["anthropic"],
							"description": "Specify which provider host this llm model"
						},
						"api_key": { 
							"type": "string", 
							"default": "",
							"description": "Specify access key for selected provider." 
						},
						"api_base": { 
							"type": "string", 
							"default": "",
							"description": "Specify the api base for selected provider. Leave it blank if you want to use default api base."
						},
						"temperature": {
							"type": "number",
							"default": 0.3,
							"description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
						},
						"max_tokens": {
							"type": "number",
							"default": 1000,
							"description": "The maximum number of tokens to generate in the chat completion.\nThe total length of input tokens and generated tokens is limited by the model's context length. Example Python code for counting tokens."
						},
						"presence_penalty": {
							"type": "number",
							"default": 0,
							"description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."
						},
						"frequency_penalty": {
							"type": "number",
							"default": 0,
							"description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
						}
					},
					"additionalProperties": false,
					"order": 3,
					"markdownDescription": "properties for claude-2 model. Leave it blank if you won't use this llm model. [how to set?](https://platform.openai.com/docs/api-reference/chat/create#temperature) "
				},

				"devchat.customModel": {
					"type": "array",
					"items": {
						"type": "object",
						"properties": {
							"model": {
								"oneOf": [
									{
										"type": "string",
										"enum": ["openai/gpt-4", "openai/gpt-4-0613", "openai/gpt-4-0314", "openai/gpt-4-32k", "openai/gpt-4-32k-0314", "openai/gpt-4-32k-0613", "openai/gpt-3.5-turbo", "openai/gpt-3.5-turbo-0301", "openai/gpt-3.5-turbo-0613", "openai/gpt-3.5-turbo-16k", "openai/gpt-3.5-turbo-16k-0613", "openai/text-davinci-003", "openai/curie-001", "openai/babbage-001", "openai/ada-001", "openai/babbage-002", "openai/davinci-002", "cohere/command-nightly", "cohere/command", "cohere/command-light", "cohere/command-medium-beta", "cohere/command-xlarge-beta", "anthropic/claude-2", "anthropic/claude-instant-1", "anthropic/claude-instant-1.2", "replicate/replicate/", "replicate/replicate/llama-2-70b-chat:58d078176e02c219e11eb4da5a02a7830a283b14cf8f94537af893ccff5ee781", "replicate/a16z-infra/llama-2-13b-chat:2a7f981751ec7fdf87b5b91ad4db53683a98082e9ff7bfd12c8cd5ea85980a52", "replicate/joehoover/instructblip-vicuna13b:c4c54e3c8c97cd50c2d2fec9be3b6065563ccf7d43787fb99f84151b867178fe", "replicate/replicate/dolly-v2-12b:ef0e1aefc61f8e096ebe4db6b2bacc297daf2ef6899f0f7e001ec445893500e5", "replicate/a16z-infra/llama-2-7b-chat:7b0bfc9aff140d5b75bacbed23e91fd3c34b01a1e958d32132de6e0a19796e2c", "replicate/replicate/vicuna-13b:6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b", "replicate/daanelson/flan-t5-large:ce962b3f6792a57074a601d3979db5839697add2e4e02696b3ced4c022d4767f", "replicate/replit/replit-code-v1-3b:b84f4c074b807211cd75e3e8b1589b6399052125b4c27106e43d47189e8415ad", "huggingface/meta-llama/Llama-2-7b-hf", "huggingface/meta-llama/Llama-2-7b-chat-hf", "huggingface/meta-llama/Llama-2-13b-hf", "huggingface/meta-llama/Llama-2-13b-chat-hf", "huggingface/meta-llama/Llama-2-70b-hf", "huggingface/meta-llama/Llama-2-70b-chat-hf", "huggingface/meta-llama/Llama-2-7b", "huggingface/meta-llama/Llama-2-7b-chat", "huggingface/meta-llama/Llama-2-13b", "huggingface/meta-llama/Llama-2-13b-chat", "huggingface/meta-llama/Llama-2-70b", "huggingface/meta-llama/Llama-2-70b-chat", "together_ai/togethercomputer/llama-2-70b-chat", "together_ai/togethercomputer/Llama-2-7B-32K-Instruct", "together_ai/togethercomputer/llama-2-7b", "baseten/qvv0xeq", "baseten/q841o8w", "baseten/31dxrj3", "openrouter/google/palm-2-codechat-bison", "openrouter/google/palm-2-chat-bison", "openrouter/openai/gpt-3.5-turbo", "openrouter/openai/gpt-3.5-turbo-16k", "openrouter/openai/gpt-4-32k", "openrouter/anthropic/claude-2", "openrouter/anthropic/claude-instant-v1", "openrouter/meta-llama/llama-2-13b-chat", "openrouter/meta-llama/llama-2-70b-chat", "vertex_ai/chat-bison", "vertex_ai/chat-bison@001", "vertex_ai/text-bison", "vertex_ai/text-bison@001", "ai21/j2-ultra", "ai21/j2-mid", "ai21/j2-light"],
										"description": "Specify llm model name."
									}, {
										"type": "string",
										"description": "Specify llm model name."
									}
								]
							},
							"api_key": { 
								"type": "string", 
								"default": "",
								"description": "Specify access key for selected provider." 
							},
							"api_base": { 
								"type": "string", 
								"default": "",
								"description": "Specify the api base for selected provider. Leave it blank if you want to use default api base."
							},
							"temperature": {
								"type": "number",
								"default": 0.3,
								"description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
							},
							"max_tokens": {
								"type": "number",
								"default": 1000,
								"description": "The maximum number of tokens to generate in the chat completion.\nThe total length of input tokens and generated tokens is limited by the model's context length. Example Python code for counting tokens."
							},
							"presence_penalty": {
								"type": "number",
								"default": 0,
								"description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."
							},
							"frequency_penalty": {
								"type": "number",
								"default": 0,
								"description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
							}
						},
						"additionalProperties": false
					},
					"order": 6,
					"markdownDescription": "Specify the custom llm model for DevChat."
				},

				"devchat.defaultModel": {
					"oneOf": [
						{
							"type": "string",
							"default": "gpt-3.5-turbo",
							"enum": [
								"gpt-3.5-turbo",
								"gpt-3.5-turbo-16k",
								"gpt-4",
								"claude-2"
							]
						}, {
							"type": "string",
							"default": "gpt-3.5-turbo"
						}
					],
					"order": 7,
					"markdownDescription": "Specify the default llm model for DevChat. [Price of each model](https://devchat.ai/pricing)"
				},


				"DevChat.activeModelKey": {
					"type": "string",
					"default": "",
					"description": "api key for llm model"
				},


				


				"DevChat.llmModel": {
					"type": "string",
					"default": "OpenAI",
					"enum": [
						"OpenAI"
					],
					"description": "Select which LLM to use."
				},
				"DevChat.maxLogCount": {
					"type": "number",
					"default": 20,
					"description": "Limit the number of prompts in the chat view."
				},
				"DevChat.OpenAI.temperature": {
					"type": "number",
					"default": 0,
					"description": "The sampling temperature to use, between 0 and 2. Lower values like 0.2 will make it more focused and deterministic.",
					"when": "DevChat.llmModel == 'OpenAI'"
				},
				"DevChat.OpenAI.stream": {
					"type": "boolean",
					"default": true,
					"description": "Whether to stream a response.",
					"when": "DevChat.llmModel == 'OpenAI'"
				},
				"DevChat.OpenAI.tokensPerPrompt": {
					"type": "number",
					"default": 6000,
					"description": "The max number of tokens of a prompt.",
					"when": "DevChat.llmModel == 'OpenAI'"
				},
				"DevChat.Access_Key_DevChat": {
					"type": "string",
					"default": "",
					"description": "DevChat's secret key for accessing multiple LLM models"
				},
				"DevChat.Api_Key_OpenAI": {
					"type": "string",
					"default": "",
					"description": "OpenAI's secret key for accessing LLM models. (Leave blank if using DevChat's key.)",
					"when": "DevChat.llmModel == 'OpenAI'"
				},
				"DevChat.API_ENDPOINT": {
					"type": "string",
					"default": "",
					"description": "API endpoint URL",
					"when": "DevChat.llmModel == 'OpenAI'"
				},
				"DevChat.DevChatPath": {
					"type": "string",
					"default": "",
					"input": {
						"type": "file",
						"filter": {
							"All files": [
								"devchat"
							]
						}
					},
					"description": "Where is the devchat binary located?"
				},
				"DevChat.PythonPath": {
					"type": "string",
					"default": "",
					"input": {
						"type": "file",
						"filter": {
							"All files": [
								"python*"
							]
						}
					},
					"description": "Which Python interpreter to use with DevChat?"
				},
				"DevChat.EnableFunctionCalling": {
					"type": "boolean",
					"default": false,
					"description": "Enable/Disable function calling for GPT.",
					"when": "DevChat.llmModel == 'OpenAI'"
				},
				"DevChat.PythonVirtualEnv": {
					"type": "string",
					"default": "",
					"description": "Path to the Python virtual environment for AskCode."
				},
				"DevChat.askcode.supportedFileTypes": {
					"type": "string",
					"default": ".+\\.js$, .+\\.ts$, .+\\.jsx$, .+\\.tsx$, .+\\.java$, .+\\.py$, .+\\.go$, .+\\.rb$, .+\\.php$, .+\\.cpp$, .+\\.c$, .+\\.cs$, .+\\.swift$, .+\\.rs$, .+\\.sh$, .+\\.bash$, .+\\.zsh$, .+\\.m$, .+\\.mm$, .+\\.h$, .+\\.hpp$, .+\\.hh$, .+\\.html$, .+\\.htm$, .+\\.xhtml$, .+\\.xml$, .+\\.css$, .+\\.scss$, .+\\.sass$, .+\\.less$, .+\\.json$, .+\\.yaml$, .+\\.yml$, .+\\.toml$, .+\\.ini$, .+\\.md$, .+\\.markdown$, .+\\.txt$, .+\\.csv$, .+\\.sql$, .+\\.sqlite$, .+\\.db$, .+\\.hql$, .+\\.psql$, .+\\.pgsql$, .+\\.plpgsql$",
					"description": "Comma-separated list of regular expressions for supported file types for analysis."
				},
				"DevChat.betaInvitationCode": {
					"type": "string",
					"default": "",
					"description": "The invitation code for beta testing."
				}
			}
		},
		"viewsContainers": {
			"activitybar": [
				{
					"id": "devchat-sidebar",
					"title": "DevChat",
					"icon": "assets/devchat_icon.svg"
				}
			]
		},
		"views": {
			"devchat-sidebar": [
				{
					"type": "webview",
					"id": "devchat-view",
					"name": "DevChat"
				},
				{
					"type": "tree",
					"id": "devchat-topicview",
					"name": "DevChat-Topic"
				}
			]
		},
		"commands": [
			{
				"command": "devchat-topicview.reloadTopic",
				"title": "Reload Topics",
				"icon": "$(refresh)"
			},
			{
				"command": "devchat-topicview.selectTopic",
				"title": "Select Topic",
				"icon": "$(add)"
			},
			{
				"command": "devchat-topicview.addTopic",
				"title": "Add Topic",
				"icon": "$(add)"
			},
			{
				"command": "devchat-topicview.deleteSelectedTopic",
				"title": "Delete Selected Topic",
				"icon": "$(trash)"
			},
			{
				"command": "devchat-topicview.deleteTopic",
				"title": "Delete topic"
			},
			{
				"command": "devchat.applyDiffResult",
				"title": "Apply Diff",
				"icon": "assets/devchat_apply.svg"
			},
			{
				"command": "devchat.createEntry",
				"title": "Create Entry"
			},
			{
				"command": "DevChat.Api_Key_OpenAI",
				"title": "Input OpenAI Api Key",
				"category": "DevChat"
			},
			{
				"command": "DevChat.Access_Key_DevChat",
				"title": "Input DevChat Access Key",
				"category": "DevChat"
			},
			{
				"command": "DevChat.PythonPath",
				"title": "Set Python Path",
				"category": "DevChat"
			},
			{
				"command": "devchat.openChatPanel",
				"title": "DevChat"
			},
			{
				"command": "devchat.addConext",
				"title": "Add to DevChat"
			},
			{
				"command": "devchat.askForCode",
				"title": "Add to DevChat"
			},
			{
				"command": "devchat.askForFile",
				"title": "Add to DevChat"
			},
			{
				"command": "devchat.addConext_chinese",
				"title": "添加到DevChat"
			},
			{
				"command": "devchat.askForCode_chinese",
				"title": "添加到DevChat"
			},
			{
				"command": "devchat.askForFile_chinese",
				"title": "添加到DevChat"
			},
			{
				"command": "DevChat.AskCodeIndexStart",
				"title": "Start AskCode Index",
				"category": "DevChat"
			},
			{
				"command": "DevChat.AskCodeIndexStop",
				"title": "Stop AskCode Index",
				"category": "DevChat"
			},
			{
				"command": "DevChat.InstallCommands",
				"title": "Install slash commands",
				"category": "DevChat"
			}
		],
		"menus": {
			"view/item/context": [
				{
					"command": "devchat-topicview.deleteTopic",
					"when": "view == devchat-topicview",
					"group": "1_modification"
				}
			],
			"view/title": [
				{
					"command": "devchat-topicview.addTopic",
					"when": "view == devchat-topicview",
					"group": "navigation"
				},
				{
					"command": "devchat-topicview.deleteSelectedTopic",
					"when": "view == devchat-topicview",
					"group": "navigation"
				},
				{
					"command": "devchat-topicview.reloadTopic",
					"when": "view == devchat-topicview",
					"group": "navigation"
				}
			],
			"editor/title": [
				{
					"command": "devchat.applyDiffResult",
					"group": "navigation",
					"when": "editorTextFocus && isInDiffEditor"
				}
			],
			"commandPalette": [
				{
					"command": "devchat-topicview.reloadTopic",
					"when": "false"
				},
				{
					"command": "devchat-topicview.selectTopic",
					"when": "false"
				},
				{
					"command": "devchat-topicview.addTopic",
					"when": "false"
				},
				{
					"command": "devchat-topicview.deleteSelectedTopic",
					"when": "false"
				},
				{
					"command": "devchat-topicview.deleteTopic",
					"when": "false"
				},
				{
					"command": "devchat.applyDiffResult",
					"when": "false"
				},
				{
					"command": "devchat.addConext",
					"when": "false"
				},
				{
					"command": "devchat.askForCode",
					"when": "false"
				},
				{
					"command": "devchat.askForFile",
					"when": "false"
				},
				{
					"command": "devchat.addConext_chinese",
					"when": "false"
				},
				{
					"command": "devchat.askForCode_chinese",
					"when": "false"
				},
				{
					"command": "devchat.askForFile_chinese",
					"when": "false"
				}
			],
			"explorer/context": [
				{
					"when": "isChineseLocale && resourceLangId != 'git'",
					"command": "devchat.addConext_chinese",
					"group": "navigation"
				},
				{
					"when": "!isChineseLocale && resourceLangId != 'git'",
					"command": "devchat.addConext",
					"group": "navigation"
				}
			],
			"editor/context": [
				{
					"command": "devchat.askForCode_chinese",
					"when": "isChineseLocale && editorTextFocus && editorHasSelection",
					"group": "navigation"
				},
				{
					"command": "devchat.askForCode",
					"when": "!isChineseLocale && editorTextFocus && editorHasSelection",
					"group": "navigation"
				},
				{
					"command": "devchat.askForFile",
					"when": "!isChineseLocale && editorTextFocus && !editorHasSelection",
					"group": "navigation"
				},
				{
					"command": "devchat.askForFile_chinese",
					"when": "isChineseLocale && editorTextFocus && !editorHasSelection",
					"group": "navigation"
				}
			]
		}
	},
	"scripts": {
		"vscode:uninstall": "node ./dist/uninstall",
		"vscode:prepublish": "npm run package",
		"compile": "webpack",
		"watch": "webpack --watch",
		"package": "webpack --mode production --devtool hidden-source-map",
		"compile-tests": "tsc -p . --outDir out",
		"watch-tests": "tsc -p . -w --outDir out",
		"pretest": "npm run compile-tests && npm run compile && npm run lint",
		"lint": "eslint src --ext ts",
		"test": "mocha",
		"build": "webpack --config webpack.config.js",
		"dev": "webpack serve --config webpack.config.js --open"
	},
	"devDependencies": {
		"@babel/core": "^7.21.8",
		"@babel/preset-env": "^7.21.5",
		"@babel/preset-react": "^7.18.6",
		"@babel/preset-typescript": "^7.21.5",
		"@types/chai": "^4.3.5",
		"@types/glob": "^8.1.0",
		"@types/mocha": "^10.0.1",
		"@types/mock-fs": "^4.13.1",
		"@types/ncp": "^2.0.5",
		"@types/node": "16.x",
		"@types/proxyquire": "^1.3.28",
		"@types/react-dom": "^18.2.3",
		"@types/react-syntax-highlighter": "^15.5.6",
		"@types/shell-escape": "^0.2.1",
		"@types/sinon": "^10.0.15",
		"@types/uuid": "^9.0.1",
		"@types/vscode": "^1.75.0",
		"@typescript-eslint/eslint-plugin": "^5.56.0",
		"@typescript-eslint/parser": "^5.56.0",
		"@vscode/test-electron": "^2.3.0",
		"babel-loader": "^9.1.2",
		"chai": "^4.3.7",
		"copy-webpack-plugin": "^11.0.0",
		"css-loader": "^6.7.3",
		"dotenv": "^16.0.3",
		"eslint": "^8.36.0",
		"file-loader": "^6.2.0",
		"glob": "^8.1.0",
		"html-webpack-plugin": "^5.5.1",
		"jest": "^29.5.0",
		"json-loader": "^0.5.7",
		"mocha": "^10.2.0",
		"mock-fs": "^5.2.0",
		"proxyquire": "^2.1.3",
		"react": "^18.2.0",
		"react-dom": "^18.2.0",
		"sinon": "^15.1.0",
		"style-loader": "^3.3.2",
		"ts-jest": "^29.1.0",
		"ts-loader": "^9.4.2",
		"ts-node": "^10.9.1",
		"typescript": "^4.9.5",
		"url-loader": "^4.1.1",
		"vscode-test": "^1.6.1",
		"webpack": "^5.76.3",
		"webpack-cli": "^5.0.1",
		"webpack-dev-server": "^4.13.3"
	},
	"dependencies": {
		"@emotion/react": "^11.10.8",
		"@mantine/core": "^6.0.10",
		"@mantine/dropzone": "^6.0.10",
		"@mantine/hooks": "^6.0.10",
		"@mantine/prism": "^6.0.10",
		"@mantine/tiptap": "^6.0.10",
		"@tabler/icons-react": "^2.17.0",
		"@tiptap/extension-link": "^2.0.3",
		"@tiptap/pm": "^2.0.0",
		"@tiptap/react": "^2.0.3",
		"@tiptap/starter-kit": "^2.0.3",
		"axios": "^1.3.6",
		"dotenv": "^16.0.3",
		"mobx": "^6.10.0",
		"mobx-react": "^9.0.0",
		"mobx-state-tree": "^5.1.8",
		"ncp": "^2.0.0",
		"node-fetch": "^3.3.1",
		"nonce": "^1.0.4",
		"openai": "^3.2.1",
		"quote": "^0.4.0",
		"react-markdown": "^8.0.7",
		"react-syntax-highlighter": "^15.5.0",
		"rehype-raw": "^6.1.1",
		"shell-escape": "^0.2.0",
		"string-argv": "^0.3.2",
		"uuid": "^9.0.0"
	}
}